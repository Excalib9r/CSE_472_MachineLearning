{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import struct\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.t = 0\n",
    "    \n",
    "    def update(self, param, grad):\n",
    "        if self.m is None:\n",
    "            self.m = np.zeros_like(param)\n",
    "            self.v = np.zeros_like(param)\n",
    "        \n",
    "        self.t += 1\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * grad\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * grad ** 2\n",
    "        m_hat = self.m / (1 - self.beta1 ** self.t)\n",
    "        v_hat = self.v / (1 - self.beta2 ** self.t)\n",
    "        param -= self.learning_rate * m_hat / (np.sqrt(v_hat) + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layers:\n",
    "    def __init__(self, numNodesIn, numNodesOut, drop_out_prob = None, activation='relu',  learning_rate=0.0001, beta1=0.9, beta2=0.999):\n",
    "        self.activation = activation\n",
    "        self.w = np.random.randn(numNodesIn, numNodesOut) * np.sqrt(2. / numNodesIn)\n",
    "        self.b = np.zeros((1, numNodesOut))  \n",
    "        self.drop_out_prob = drop_out_prob\n",
    "        self.dropout_mask = None\n",
    "        self.prev_input = None\n",
    "        self.z = None\n",
    "        self.a = None\n",
    "        self.delta = None\n",
    "        self.gamma = np.ones((1, numNodesOut))  \n",
    "        self.beta = np.zeros((1, numNodesOut)) \n",
    "        self.n = None\n",
    "        self.x_hat = None\n",
    "        self.mean = None\n",
    "        self.variance = None\n",
    "        self.out = None\n",
    "        self.cache = None\n",
    "        self.training = True  \n",
    "        self.running_mean = np.zeros((1, numNodesOut))\n",
    "        self.running_var = np.zeros((1, numNodesOut))\n",
    "\n",
    "        self.w_optimizer = AdamOptimizer(learning_rate, beta1, beta2)\n",
    "        self.b_optimizer = AdamOptimizer(learning_rate, beta1, beta2)\n",
    "        self.gamma_optimizer = AdamOptimizer(learning_rate, beta1, beta2)\n",
    "        self.beta_optimizer = AdamOptimizer(learning_rate, beta1, beta2)\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "    \n",
    "    def set_training_mode(self, training=True):\n",
    "        self.training = training\n",
    "\n",
    "    def batchnorm_forward(self, z):\n",
    "        N, D = z.shape\n",
    "        if self.training:\n",
    "            mu = np.mean(z, axis=0, keepdims=True)\n",
    "            var = np.var(z, axis=0, keepdims=True)\n",
    "            xhat = (z - mu) / np.sqrt(var + 1e-8)\n",
    "            self.out = self.gamma * xhat + self.beta\n",
    "            momentum = 0.9\n",
    "            self.running_mean = momentum * self.running_mean + (1 - momentum) * mu\n",
    "            self.running_var = momentum * self.running_var + (1 - momentum) * var\n",
    "            self.cache = (xhat, z - mu, 1. / np.sqrt(var + 1e-8), np.sqrt(var + 1e-8), var)\n",
    "        else:\n",
    "            xhat = (z - self.running_mean) / np.sqrt(self.running_var + 1e-8)\n",
    "            self.out = self.gamma * xhat + self.beta\n",
    "        return self.out\n",
    "    \n",
    "    def dropout(self, x):\n",
    "        if self.drop_out_prob is not None and self.training:\n",
    "            self.dropout_mask = (np.random.rand(1, x.shape[1]) > self.drop_out_prob).astype(float)\n",
    "            return x * self.dropout_mask / (1 - self.drop_out_prob)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.prev_input = input  \n",
    "        self.z = np.dot(input, self.w) + self.b  \n",
    "        if self.activation == 'relu':\n",
    "            self.n = self.batchnorm_forward(self.z)  \n",
    "            self.a = self.dropout(self.relu(self.n))  \n",
    "        elif self.activation == 'softmax':\n",
    "            self.a = self.softmax(self.z)  \n",
    "        return self.a\n",
    "    \n",
    "    def batchnorm_backward(self, dout):\n",
    "        xhat, xmu, ivar, sqrtvar, var = self.cache\n",
    "        N, D = dout.shape\n",
    "\n",
    "        dbeta = np.sum(dout, axis=0, keepdims=True)\n",
    "        dgamma = np.sum(dout * xhat, axis=0, keepdims=True)\n",
    "        dxhat = dout * self.gamma\n",
    "        dvar = np.sum(dxhat * xmu * -0.5 * (var + 1e-8) ** (-1.5), axis=0, keepdims=True)\n",
    "        dmu = np.sum(dxhat * -ivar, axis=0, keepdims=True) + dvar * np.mean(-2. * xmu, axis=0, keepdims=True)\n",
    "        dx = dxhat * ivar + dvar * 2 * xmu / N + dmu / N\n",
    "\n",
    "        return dx, dgamma, dbeta\n",
    "\n",
    "    def backward(self, upstream_grad):\n",
    "        if self.activation == 'relu':\n",
    "            activation_derivative = self.relu_derivative(self.n)\n",
    "\n",
    "            self.delta = upstream_grad * activation_derivative \n",
    "            if self.drop_out_prob is not None and self.training:\n",
    "                self.delta *= self.dropout_mask \n",
    "\n",
    "            dz, dgamma, dbeta = self.batchnorm_backward(self.delta) \n",
    "\n",
    "           \n",
    "            gradient_w = np.dot(self.prev_input.T, dz) / self.prev_input.shape[0]  \n",
    "            gradient_b = np.sum(dz, axis=0, keepdims=True) / self.prev_input.shape[0]  \n",
    "\n",
    "            self.w_optimizer.update(self.w, gradient_w)\n",
    "            self.b_optimizer.update(self.b, gradient_b)\n",
    "\n",
    "            self.gamma_optimizer.update(self.gamma, dgamma / self.prev_input.shape[0])\n",
    "            self.beta_optimizer.update(self.beta, dbeta / self.prev_input.shape[0])\n",
    "\n",
    "            return np.dot(dz, self.w.T) \n",
    "\n",
    "        elif self.activation == 'softmax':\n",
    "            self.delta = upstream_grad \n",
    "\n",
    "            gradient_w = np.dot(self.prev_input.T, self.delta) / self.prev_input.shape[0]  \n",
    "            gradient_b = np.sum(self.delta, axis=0, keepdims=True) / self.prev_input.shape[0] \n",
    "\n",
    "            self.w_optimizer.update(self.w, gradient_w)\n",
    "            self.b_optimizer.update(self.b, gradient_b)\n",
    "\n",
    "            return np.dot(self.delta, self.w.T)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, layers, drop_out = None, hidden_layer_activation='relu', output_layer_activation='softmax', loss_type='cross_entropy', learning_rate=0.0001, beta1 = 0.9, beta2 = 0.999):\n",
    "        self.loss_type = loss_type\n",
    "        self.layers = []  \n",
    "        for i in range(len(layers) - 1):  \n",
    "            if i == len(layers) - 2: \n",
    "                self.layers.append(Layers(layers[i], layers[i + 1],  drop_out_prob=drop_out, activation=output_layer_activation, learning_rate=learning_rate, beta1=beta1, beta2=beta2))\n",
    "            else:\n",
    "                self.layers.append(Layers(layers[i], layers[i + 1], activation=hidden_layer_activation,  learning_rate=learning_rate, beta1=beta1, beta2=beta2)) \n",
    "        self.output = None\n",
    "    \n",
    "    def forward(self, input, training=True):\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'set_training_mode'):\n",
    "                layer.set_training_mode(training)\n",
    "            input = layer.forward(input)\n",
    "        self.output = input\n",
    "        return self.output\n",
    "    \n",
    "    def classify(self, input):\n",
    "        self.output = self.forward(input, training=False)\n",
    "        return np.argmax(self.output, axis=1)\n",
    "    \n",
    "    def calculateCost(self, target):\n",
    "        output = self.output\n",
    "        epsilon = 1e-12\n",
    "        output = np.clip(output, epsilon, 1. - epsilon)\n",
    "        return -np.sum(target * np.log(output)) / target.shape[0]\n",
    "    \n",
    "    def backward(self, target):\n",
    "        loss = self.output - target\n",
    "        for layer in reversed(self.layers):\n",
    "            loss = layer.backward(loss)\n",
    "\n",
    "    def save_parameters(self, file_path):\n",
    "        parameters = {}\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            parameters[f'layer_{idx}'] = {\n",
    "                'w': layer.w,\n",
    "                'b': layer.b,\n",
    "                'gamma': getattr(layer, 'gamma', None),\n",
    "                'beta': getattr(layer, 'beta', None),\n",
    "                'running_mean': layer.running_mean,\n",
    "                'running_var': layer.running_var\n",
    "            }\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(parameters, f)\n",
    "\n",
    "    def load_parameters(self, file_path):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            parameters = pickle.load(f)\n",
    "            for idx, layer in enumerate(self.layers):\n",
    "                layer_params = parameters.get(f'layer_{idx}', {})\n",
    "                if 'w' in layer_params:\n",
    "                    layer.w = layer_params['w']\n",
    "                if 'b' in layer_params:\n",
    "                    layer.b = layer_params['b']\n",
    "                if 'gamma' in layer_params and layer_params['gamma'] is not None:\n",
    "                    layer.gamma = layer_params['gamma']\n",
    "                if 'beta' in layer_params and layer_params['beta'] is not None:\n",
    "                    layer.beta = layer_params['beta']\n",
    "                if 'running_mean' in layer_params:\n",
    "                    layer.running_mean = layer_params['running_mean']\n",
    "                if 'running_var' in layer_params:\n",
    "                    layer.running_var = layer_params['running_var']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_images(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        magic, num, rows, cols = struct.unpack('>IIII', f.read(16))\n",
    "        if magic != 2051:\n",
    "            raise ValueError(f'Invalid magic number {magic} in image file: {file_path}')\n",
    "        images = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        images = images.reshape(num, rows * cols)\n",
    "        images = images.astype(np.float32) / 255.0 \n",
    "    return images\n",
    "\n",
    "def load_labels(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        magic, num = struct.unpack('>II', f.read(8))\n",
    "        if magic != 2049:\n",
    "            raise ValueError(f'Invalid magic number {magic} in label file: {file_path}')\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    return labels\n",
    "\n",
    "train_images = load_images('data/FashionMNIST/raw/train-images-idx3-ubyte')\n",
    "train_labels = load_labels('data/FashionMNIST/raw/train-labels-idx1-ubyte')\n",
    "test_images = load_images('data/FashionMNIST/raw/t10k-images-idx3-ubyte')\n",
    "test_labels = load_labels('data/FashionMNIST/raw/t10k-labels-idx1-ubyte')\n",
    "\n",
    "\n",
    "def create_batches(images, labels, batch_size=64, shuffle=True):\n",
    "    num_samples = images.shape[0]\n",
    "    indices = np.arange(num_samples)\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    for start_idx in range(0, num_samples, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, num_samples)\n",
    "        batch_indices = indices[start_idx:end_idx]\n",
    "        yield images[batch_indices], labels[batch_indices]\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "train_loader = list(create_batches(train_images, train_labels, batch_size=batch_size, shuffle=True))\n",
    "test_loader = list(create_batches(test_images, test_labels, batch_size=batch_size, shuffle=False))\n",
    "\n",
    "def count_batches(generator):\n",
    "    count = 0\n",
    "    for _ in generator:\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers = [784, 128, 64, 10] \n",
    "# network = Network(layers, drop_out=0.3, learning_rate=0.0001)\n",
    "\n",
    "# save_path = '1905080.pkl'\n",
    "\n",
    "# train_costs = []\n",
    "# train_accuracies = []\n",
    "\n",
    "# infer_cost = []\n",
    "# infer_accuracies = []\n",
    "\n",
    "# val_labels = []\n",
    "# val_predictions = []\n",
    "\n",
    "# f1_scores = []\n",
    "\n",
    "# for epoch in range(25):\n",
    "#     correct_train = 0\n",
    "#     total_train = 0\n",
    "#     epoch_cost_train = 0\n",
    "\n",
    "#     val_labels = []\n",
    "#     val_predictions = []\n",
    "\n",
    "#     for images, labels in train_loader:\n",
    "#         labels_one_hot = np.eye(10)[labels]\n",
    "#         network.forward(images)\n",
    "#         cost = network.calculateCost(labels_one_hot)\n",
    "#         network.backward(labels_one_hot)\n",
    "#         predictions = network.classify(images)\n",
    "#         correct_train += np.sum(predictions == labels)\n",
    "#         total_train += len(labels)\n",
    "#         epoch_cost_train += cost\n",
    "\n",
    "#     avg_cost_train = 100 * epoch_cost_train /len(train_loader)\n",
    "#     train_costs.append(avg_cost_train)\n",
    "\n",
    "#     train_accuracy = 100 * correct_train / total_train\n",
    "#     train_accuracies.append(train_accuracy)\n",
    "    \n",
    "#     correct_infer = 0\n",
    "#     total_infer = 0\n",
    "#     epoch_cost_infer = 0\n",
    "\n",
    "#     for images, labels in test_loader:\n",
    "#         outputs = network.classify(images)\n",
    "#         correct_infer += np.sum(outputs == labels)\n",
    "#         labels_one_hot = np.eye(10)[labels]\n",
    "#         cost = network.calculateCost(labels_one_hot)\n",
    "#         total_infer += len(labels)\n",
    "#         epoch_cost_infer += cost\n",
    "\n",
    "#         val_labels.extend(labels)\n",
    "#         val_predictions.extend(outputs)\n",
    "    \n",
    "#     avg_cost_test = 100 * epoch_cost_infer / len(test_loader)\n",
    "#     infer_cost.append(avg_cost_test)\n",
    "\n",
    "#     infer_accuracy = 100 * correct_infer / total_infer\n",
    "#     infer_accuracies.append(infer_accuracy)\n",
    "\n",
    "#     f1 = 100* f1_score(val_labels, val_predictions, average='weighted')\n",
    "#     f1_scores.append(f1)\n",
    "\n",
    "# class_names = [f'Class {i}' for i in range(10)]\n",
    "# cm = confusion_matrix(val_labels, val_predictions)\n",
    "# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "#             xticklabels=class_names,\n",
    "#             yticklabels=class_names)\n",
    "# plt.xlabel('Predicted Label')\n",
    "# plt.ylabel('True Label')\n",
    "# plt.title('[784, 128, 10] - lr: 0.0001')\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(train_costs, label='Training Cost')\n",
    "# plt.plot(infer_cost, label='Inference Cost')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Cost')\n",
    "# plt.title('[784, 128, 10] - lr: 0.0001')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(train_accuracies, label='Training Accuracy')\n",
    "# plt.plot(infer_accuracies, label='Inference Accuracy')\n",
    "# plt.plot(f1_scores, label='F1 Score')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('[784, 128, 10] - lr: 0.0001')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# network.save_parameters(save_path)\n",
    "# print(f\"Parameters saved successfully at '{save_path}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 36.60959061202709\n",
      "Accuracy: 88.07%\n",
      "F1 Score: 88.13611184014867%\n"
     ]
    }
   ],
   "source": [
    "layers = [784, 128, 64, 10] \n",
    "network = Network(layers, drop_out=0.3, learning_rate=0.0001)\n",
    "model_path = '1905080.pkl'\n",
    "network.load_parameters(model_path)\n",
    "\n",
    "correct_infer = 0\n",
    "total_infer = 0\n",
    "epoch_cost_infer = 0\n",
    "\n",
    "val_labels = []\n",
    "val_predictions = []\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    outputs = network.classify(images)\n",
    "    correct_infer += np.sum(outputs == labels)\n",
    "    labels_one_hot = np.eye(10)[labels]\n",
    "    cost = network.calculateCost(labels_one_hot)\n",
    "    total_infer += len(labels)\n",
    "    epoch_cost_infer += cost\n",
    "\n",
    "    val_labels.extend(labels)\n",
    "    val_predictions.extend(outputs)\n",
    "\n",
    "avg_cost_test = 100 * epoch_cost_infer / len(test_loader)\n",
    "print(f'Cost: {avg_cost_test}')\n",
    "\n",
    "infer_accuracy = 100 * correct_infer / total_infer\n",
    "print(f'Accuracy: {infer_accuracy}%')\n",
    "\n",
    "f1 = 100* f1_score(val_labels, val_predictions, average='weighted')\n",
    "print(f'F1 Score: {f1}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
