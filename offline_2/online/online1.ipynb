{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, average_precision_score, confusion_matrix\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__ (self, lr=0.0001, iter=1000, tol=1e-6):\n",
    "        self.lr = lr\n",
    "        self.iter = iter\n",
    "        self.weights = None\n",
    "        self.tol = tol\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def update_weights(self, X, y):\n",
    "        y_pred = self.sigmoid(np.dot(X, self.weights))\n",
    "        error = y - y_pred\n",
    "        gradient = np.dot(X.T, error)\n",
    "        self.weights += self.lr * gradient\n",
    "\n",
    "    def train(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        X = np.hstack((np.ones((n_samples, 1)), X))\n",
    "        self.weights = np.zeros(n_features + 1)\n",
    "        for _ in range(self.iter):\n",
    "            self.update_weights(X, y)\n",
    "            \n",
    "    def predict(self, X, final=True):\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        if final:\n",
    "            return np.round(self.sigmoid(np.dot(X, self.weights)))\n",
    "        return self.sigmoid(np.dot(X, self.weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bagging:\n",
    "    def __init__ (self, base_estimator = LogisticRegression, n_estimators=9, random_state=40, sample_size=1.0):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.random_state = random_state\n",
    "        self.sample_size = sample_size\n",
    "        self.estimators_ = []\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        self.estimators_ = []\n",
    "        np.random.seed(self.random_state)  \n",
    "        for _ in range(self.n_estimators):\n",
    "            estimator = self.base_estimator()\n",
    "            X_resampled, y_resampled = resample(X, y, n_samples=int(self.sample_size * X.shape[0]))\n",
    "            estimator.train(X_resampled, y_resampled)\n",
    "            self.estimators_.append(estimator)\n",
    "    \n",
    "    def predict(self, X, final=True):\n",
    "        predictions = []\n",
    "        for estimator in self.estimators_:\n",
    "            predictions.append(estimator.predict(X, final))\n",
    "        predictions = np.array(predictions)\n",
    "        return predictions\n",
    "    \n",
    "    def predict_majority(self, X, final=True):\n",
    "        predictions = self.predict(X, final)\n",
    "        majority_votes = []\n",
    "        for i in range(predictions.shape[1]):\n",
    "            feature_predictions = predictions[:, i].astype(int)\n",
    "            majority_vote = np.argmax(np.bincount(feature_predictions))\n",
    "            majority_votes.append(majority_vote)\n",
    "        return np.array(majority_votes)\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        predictions = self.predict(X, final=False)\n",
    "        return np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stacking:\n",
    "    def __init__ (self, base_estimator = LogisticRegression, meta_estimator= LogisticRegression):\n",
    "        self.base_estimators = base_estimator\n",
    "        self.meta_estimator = meta_estimator\n",
    "        self.meta_model = None\n",
    "        self.base_models = None\n",
    "\n",
    "    def train(self, X, y, X_meta, y_meta):\n",
    "        self.base_models = Bagging(self.base_estimators, n_estimators=9)\n",
    "        self.base_models.train(X, y)\n",
    "        meta_samples = self.base_models.predict(X_meta, False).T\n",
    "        meta_samples = np.hstack([ X_meta , meta_samples])\n",
    "        self.meta_model = self.meta_estimator()\n",
    "        self.meta_model.train(meta_samples, y_meta)\n",
    "\n",
    "    def predict(self, X, final=True):\n",
    "        meta_samples = self.base_models.predict(X, False).T\n",
    "        meta_samples = np.hstack([X, meta_samples])\n",
    "        prediction = self.meta_model.predict(meta_samples, final)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 3)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv('B1.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [X1, X2, y]\n",
       "Index: []"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [X1, X2, y]\n",
       "Index: []"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'X1' has values [ 0.02114873  0.17373477  0.25104132 ... -0.06410917  0.77160769\n",
      " -0.79093564]\n",
      "Column 'X2' has values [ 0.41824576 -0.99352424  0.44758846 ... -1.0313122  -0.79397815\n",
      " -0.37937787]\n",
      "Column 'y' has values [1 0]\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    unique_values = df[column].unique()\n",
    "    print(f\"Column '{column}' has values {unique_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X1    float64\n",
       "X2    float64\n",
       "y       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['X1', 'X2']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['y'].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scaling(scalingmethod):\n",
    "    if scalingmethod == 'minmax':\n",
    "        return MinMaxScaler()\n",
    "    elif scalingmethod == 'standard':\n",
    "        return StandardScaler()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid scaling method. Choose 'minmax' or 'standard'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = scaling('standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code from implemented LR and Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My implemented LR and Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression classifier: 0.5617\n",
      "Sensitivity (Recall): 0.9118\n",
      "Specificity: 0.2713\n",
      "Precision: 0.5092\n",
      "F1-score: 0.6535\n",
      "AUROC: 0.4559\n",
      "AUPR: 0.3892\n"
     ]
    }
   ],
   "source": [
    "# y_train = y_train.to_numpy().ravel()\n",
    "# y_test = y_test.to_numpy().ravel()\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# clf.fit(X_train, y_train)\n",
    "# y_pred = clf.predict(X_test)\n",
    "# y_pred_prob = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "clf.train(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred_prob = clf.predict(X_test, False)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "sensitivity = recall_score(y_test, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auroc = roc_auc_score(y_test, y_pred_prob)\n",
    "aupr = average_precision_score(y_test, y_pred_prob)\n",
    "\n",
    "print(f\"Accuracy of Logistic Regression classifier: {accuracy:.4f}\")\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(f\"AUROC: {auroc:.4f}\")\n",
    "print(f\"AUPR: {aupr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression classifier: 0.5617\n",
      "Sensitivity (Recall): 0.9118\n",
      "Specificity: 0.2713\n",
      "Precision: 0.5092\n",
      "F1-score: 0.6535\n",
      "AUROC: 0.4559\n",
      "AUPR: 0.3892\n"
     ]
    }
   ],
   "source": [
    "# y_train = y_train.to_numpy().ravel()\n",
    "# y_test = y_test.to_numpy().ravel()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# clf = LogisticRegression()\n",
    "\n",
    "# clf.fit(X_train, y_train)\n",
    "# y_pred = clf.predict(X_test)\n",
    "# y_pred_prob = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "clf.train(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred_prob = clf.predict(X_test, False)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "sensitivity = recall_score(y_test, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auroc = roc_auc_score(y_test, y_pred_prob)\n",
    "aupr = average_precision_score(y_test, y_pred_prob)\n",
    "\n",
    "print(f\"Accuracy of Logistic Regression classifier: {accuracy:.4f}\")\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(f\"AUROC: {auroc:.4f}\")\n",
    "print(f\"AUPR: {aupr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Bagging: 0.5367\n",
      "Sensitivity (Recall): 0.9963\n",
      "Specificity: 0.1555\n",
      "Precision: 0.4945\n",
      "F1-score: 0.6610\n",
      "AUROC: 0.4563\n",
      "AUPR: 0.3894\n"
     ]
    }
   ],
   "source": [
    "clf = Bagging()\n",
    "clf.train(X_train, y_train)\n",
    "y_pred = clf.predict_majority(X_test)\n",
    "y_pred_prob = clf.predict_prob(X_test)\n",
    "\n",
    "majority_accuracy = accuracy_score(y_test, y_pred)\n",
    "majority_sensitivity = recall_score(y_test, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "majority_specificity = tn / (tn + fp)\n",
    "majority_precision = precision_score(y_test, y_pred)\n",
    "majority_f1 = f1_score(y_test, y_pred)\n",
    "majority_auroc = roc_auc_score(y_test, y_pred_prob)\n",
    "majority_aupr = average_precision_score(y_test, y_pred_prob)\n",
    "\n",
    "print(f\"Accuracy of Bagging: {majority_accuracy:.4f}\")\n",
    "print(f\"Sensitivity (Recall): {majority_sensitivity:.4f}\")\n",
    "print(f\"Specificity: {majority_specificity:.4f}\")\n",
    "print(f\"Precision: {majority_precision:.4f}\")\n",
    "print(f\"F1-score: {majority_f1:.4f}\")\n",
    "print(f\"AUROC: {majority_auroc:.4f}\")\n",
    "print(f\"AUPR: {majority_aupr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of VotingClassifier: 0.5667\n",
      "Sensitivity (Recall): 0.9228\n",
      "Specificity: 0.2713\n",
      "Precision: 0.5122\n",
      "F1-score: 0.6588\n",
      "AUROC: 0.4561\n",
      "AUPR: 0.3894\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, precision_score, f1_score, roc_auc_score, average_precision_score\n",
    "\n",
    "# Initialize 10 BaggingClassifiers with Logistic Regression as the base estimator\n",
    "classifiers = [('bag' + str(i), BaggingClassifier(estimator=LogisticRegression(), n_estimators=10)) for i in range(10)]\n",
    "\n",
    "# Create a VotingClassifier with 10 BaggingClassifiers\n",
    "voting_clf = VotingClassifier(estimators=classifiers, voting='soft')  # Use 'hard' for majority voting, 'soft' for weighted voting\n",
    "\n",
    "# Train the VotingClassifier\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "y_pred_prob = voting_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "majority_accuracy = accuracy_score(y_test, y_pred)\n",
    "majority_sensitivity = recall_score(y_test, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "majority_specificity = tn / (tn + fp)\n",
    "majority_precision = precision_score(y_test, y_pred)\n",
    "majority_f1 = f1_score(y_test, y_pred)\n",
    "majority_auroc = roc_auc_score(y_test, y_pred_prob)\n",
    "majority_aupr = average_precision_score(y_test, y_pred_prob)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy of VotingClassifier: {majority_accuracy:.4f}\")\n",
    "print(f\"Sensitivity (Recall): {majority_sensitivity:.4f}\")\n",
    "print(f\"Specificity: {majority_specificity:.4f}\")\n",
    "print(f\"Precision: {majority_precision:.4f}\")\n",
    "print(f\"F1-score: {majority_f1:.4f}\")\n",
    "print(f\"AUROC: {majority_auroc:.4f}\")\n",
    "print(f\"AUPR: {majority_aupr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Stacking: 0.5167\n",
      "Sensitivity (Recall): 0.5037\n",
      "Specificity: 0.5274\n",
      "Precision: 0.4692\n",
      "F1-score: 0.4858\n",
      "AUROC: 0.5310\n",
      "AUPR: 0.4233\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_meta, y_train, y_meta = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = Stacking()\n",
    "clf.train(X_train, y_train, X_meta, y_meta)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred_prob = clf.predict(X_test, False)\n",
    "\n",
    "stacking_accuracy = accuracy_score(y_test, y_pred)\n",
    "stacking_sensitivity = recall_score(y_test, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "stacking_specificity = tn / (tn + fp)\n",
    "stacking_precision = precision_score(y_test, y_pred)\n",
    "stacking_f1 = f1_score(y_test, y_pred)\n",
    "stacking_auroc = roc_auc_score(y_test, y_pred_prob)\n",
    "stacking_aupr = average_precision_score(y_test, y_pred_prob)\n",
    "\n",
    "print(f\"Accuracy of Stacking: {stacking_accuracy:.4f}\")\n",
    "print(f\"Sensitivity (Recall): {stacking_sensitivity:.4f}\")\n",
    "print(f\"Specificity: {stacking_specificity:.4f}\")\n",
    "print(f\"Precision: {stacking_precision:.4f}\")\n",
    "print(f\"F1-score: {stacking_f1:.4f}\")\n",
    "print(f\"AUROC: {stacking_auroc:.4f}\")\n",
    "print(f\"AUPR: {stacking_aupr:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
